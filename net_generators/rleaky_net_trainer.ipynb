{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Setup\n",
    "\n",
    "# imports\n",
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import gc\n",
    "\n",
    "# dataloader arguments\n",
    "batch_size = 128\n",
    "data_path='/tmp/data/mnist'\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Define a transform\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize((28, 28)),\n",
    "            transforms.Grayscale(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0,), (1,))])\n",
    "\n",
    "mnist_train = datasets.MNIST(data_path, train=True, download=True, transform=transform)\n",
    "mnist_test = datasets.MNIST(data_path, train=False, download=True, transform=transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(mnist_train, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# Network Architecture\n",
    "num_inputs = 28*28\n",
    "num_hidden = 1000\n",
    "num_outputs = 10\n",
    "\n",
    "# Temporal Dynamics\n",
    "num_steps = 25\n",
    "beta = 0.70\n",
    "\n",
    "V1 = 0.5 # shared recurrent connection\n",
    "V2 = torch.rand(num_outputs) # unshared recurrent connections\n",
    "\n",
    "# Define Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # initialize layers\n",
    "        self.fc1 = nn.Linear(num_inputs, num_hidden)\n",
    "\n",
    "        # Default RLeaky Layer where recurrent connections\n",
    "        # are initialized using PyTorch defaults in nn.Linear.\n",
    "        self.lif1 = snn.RLeaky(beta=beta,\n",
    "                    linear_features=num_hidden)\n",
    "\n",
    "        self.fc2 = nn.Linear(num_hidden, num_outputs)\n",
    "\n",
    "        # each neuron has a single connection back to itself\n",
    "        # where the output spike is scaled by V.\n",
    "        # For `all_to_all = False`, V can be shared between\n",
    "        # neurons (e.g., V1) or unique / unshared between\n",
    "        # neurons (e.g., V2).\n",
    "        # V is learnable by default.\n",
    "        self.lif2 = snn.RLeaky(beta=beta, all_to_all=False, V=V1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden states at t=0\n",
    "        spk1, mem1 = self.lif1.init_rleaky()\n",
    "        spk2, mem2 = self.lif2.init_rleaky()\n",
    "\n",
    "        # Record output layer spikes and membrane\n",
    "        spk2_rec = []\n",
    "        mem2_rec = []\n",
    "\n",
    "        # time-loop\n",
    "        for step in range(num_steps):\n",
    "            cur1 = self.fc1(x)\n",
    "            spk1, mem1 = self.lif1(cur1, spk1, mem1)\n",
    "            cur2 = self.fc2(spk1)\n",
    "            spk2, mem2 = self.lif2(cur2, spk2, mem2)\n",
    "\n",
    "            spk2_rec.append(spk2)\n",
    "            mem2_rec.append(mem2)\n",
    "\n",
    "        # convert lists to tensors\n",
    "        spk2_rec = torch.stack(spk2_rec)\n",
    "        mem2_rec = torch.stack(mem2_rec)\n",
    "\n",
    "        return spk2_rec, mem2_rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
